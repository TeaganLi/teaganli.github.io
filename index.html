<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="keywords" content="tencent, robotics, Li Tingguang"><meta name="author" content="LI Tingguang"> <meta name="description" content="Personal website of Tingguang">


		
	
		<title>Tingguang Li</title>
			
			
		<!-- CSS -->
        <!-- <link href='http://fonts.googleapis.com/css?family=Quicksand:300,400' rel='stylesheet' type='text/css'>
        <link href='http://fonts.googleapis.com/css?family=Oxygen:400,300' rel='stylesheet' type='text/css'> -->
        <link href="./TingguangLi_files/css" rel="stylesheet">
		<link rel="stylesheet" href="./TingguangLi_files/style.css" type="text/css" media="screen">
        <link rel="stylesheet" href="./TingguangLi_files/jquery.popup.css" type="text/css">
		<!-- ENDS CSS -->

        <link rel="icon" href="./TingguangLi_files/cuhk.ico" type="image/x-icon">
        
        <!-- JS -->
            <!-- JS+UI -->
            <script async="" src="./TingguangLi_files/analytics.js"></script><script src="./TingguangLi_files/jquery-1.9.1.min.js"></script>
            <script src="./TingguangLi_files/jqueryui.js"></script>
            <!-- ENDS JS+UI -->
            
            <!-- Isotope -->
            <script src="./TingguangLi_files/jquery.isotope.min.js"></script>
            <!-- ENDS Isotope -->
            
            <script src="./TingguangLi_files/jquery.easing.1.3.js"></script>
            
            <!-- NProgress -->
            <link href="./TingguangLi_files/nprogress.css" rel="stylesheet">
            <script src="./TingguangLi_files/nprogress.js"></script>
            <!-- ENDS NProgress -->
            
            <!-- Popup -->
            <script type="text/javascript" src="./TingguangLi_files/jquery.popup.js"></script>
            <script type="text/javascript">$(function() {$(".js__p_start, .js__p_another_start").simplePopup();});</script>
            <!-- ENDS Popup -->
            
            <!-- Navgoco -->
                <script type="text/javascript" src="./TingguangLi_files/jquery.cookie.js"></script>
                <script type="text/javascript" src="./TingguangLi_files/jquery.navgoco.js"></script>
                <link rel="stylesheet" href="./TingguangLi_files/jquery.navgoco.css" type="text/css" media="screen">
            <!-- ENDS Navgoco -->
            
            <!-- Custom Scripts -->
            <script src="./TingguangLi_files/index-isotope.js"></script>
            <script type="text/javascript"> $(document).ready ( function(){ NProgress.start();});​ </script>
            <script type="text/javascript"> $(document).ready ( function(){$('.nav').navgoco();}); </script>   
            <!-- ENDS Custom Scripts -->
        <!-- ENDS JS -->  

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-112416327-1', 'auto');
          ga('send', 'pageview');

        </script>


	<style id="style-1-cropbar-clipper">/* Copyright 2014 Evernote Corporation. All rights reserved. */
.en-markup-crop-options {
    top: 18px !important;
    left: 50% !important;
    margin-left: -100px !important;
    width: 200px !important;
    border: 2px rgba(255,255,255,.38) solid !important;
    border-radius: 4px !important;
}

.en-markup-crop-options div div:first-of-type {
    margin-left: 0px !important;
}
</style></head>	
	<body>
		<div id="portfolio">
            <div id="portfolio-bio">
                <br><h1>Tingguang Li （李珽光）</h1>
                <h2>tgli0809 AT gmail.com</h2>
                <br>
                I am currently a Senior Research Scientist at Robotics X, Tencent. Before that, I received the Ph.D. degree at <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>, supervised by <a href="http://www.ee.cuhk.edu.hk/~qhmeng/">Professor Max Qing-Hu Meng</a>, and the BEng. from <a href="https://www.nju.edu.cn/EN/">Nanjing University</a>. In 2019, I was a Visiting Student Researcher at <a href="https://ai.stanford.edu/">Artificial Intelligence Laboratory (SAIL)</a>, Stanford University, under the supervision of <a href="https://am.is.tuebingen.mpg.de/person/jbohg">Professor Jeannette Bohg</a>.
                <br><br>
                My research interests lie in applying Reinforcement Learning on Robotics, including dexterous manipulation, quadrapdal control, decision making, imitation learning, sim-to-real transfer. Recently, I am working on projects of multimodal models, specifically, Vison Language Action models (VLA). We hope to endow robots with the intelligence to see (Vision), listen (language) and react (Action) autonomously.
                <br><br>
                !!!We are hiring self-motivated interns. If you are working on deep reinforcement learning, robotics, and interested in large models, please feel free to contact me.
                <br><br>
                <h2><a href="https://scholar.google.com.hk/citations?user=n8dcoCsAAAAJ&hl=en">Google Scholar</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a href="./TingguangLi_files/Tingguang.pdf">CV</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a href="https://github.com/TeaganLi">Github</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a href="https://hk.linkedin.com/in/tingguang-li">Linkedin</a></h2>
            </div>
            <div id="portfolio-photo">
                <img src="./TingguangLi_files/icon2.jpg">
            </div>
            <div id="portfolio-info">
                <h1>News</h1>
                <br><ul style="list-style-type:disc; margin-left: 20px">
                	<li>[09/2023] Two papers accepted in ICRA 2024.  </li>
                    <li>[06/2023] Our latest work on animal-level agility is released <a href="https://mp.weixin.qq.com/s/8hnRq_HSL0E5yGf2e6JI_w">栩栩如生的机器狗：腾讯Robotics X用预训练模型和强化学习提升机器狗控制</a>.  </li>
                </ul>

                <br><br><h1>Journal Papers</h1>
                <table id="portfolio-projects">
                    <tbody><tr style="border-width: 1px">
                        <td>
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <p>
                                <b>Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Generative Pre-trained Models</b><br>
                                Lei Han*, Qingxu Zhu*, Jiapeng Sheng*, Chong Zhang*, <b>Tingguang Li*</b>, Yizheng Zhang*, He Zhang*, Yuzhen Liu, Cheng Zhou, Rui Zhao, Jie Li, Yufeng Zhang, Rui Wang, Wanchao Chi, Xiong Li, Yonghui Zhu, Lingzhu Xiang, Xiao Teng, Zhengyou Zhang.<br> 
                                (Under review)
                                <br><a href="https://tencent-roboticsx.github.io/lifelike-agility-and-play/">Project Page</a>&nbsp;·&nbsp;<a href="https://arxiv.org/abs/2308.15143">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=ucucrLqT5dM">Video Demo (YouTube)</a>
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr></tbody></table>           
                        </td>
                    </tr> 
                    </tbody>

                    <tbody><tr style="border-width: 1px">
                        <td>
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <p>
                                <b>GMR-RRT*: Sampling-based Humanlike Path Planning Using Gaussian Mixture Regression</b><br>
                                Jiankun Wang, <b>Tingguang Li</b>, Baopu Li, Max Q.-H. Meng.<br> 
                                (IEEE Transactions on Intelligent Vehicles 2022) 
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr></tbody></table>           
                        </td>
                    </tr> 
                    </tbody>

                    <tbody><tr style="border-width: 1px">
                        <td>
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <p>
                                <b>Learning Robot Exploration Strategy with 4D Point-Clouds-like Information as Observations</b><br>
                                Zhaoting Li, <b>Tingguang Li</b>, Jiankun Wang, Max Q.-H. Meng.<br> 
                                (IEEE Robotics and Automation Letters 2021) 
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr></tbody></table>           
                        </td>
                    </tr> 
                    </tbody>


                    <tbody><tr style="border-width: 1px">
                        <td>
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <p>
                                <b>An Autonomous Eye-in-hand Robotic System for Elevator Button Operation Based on Deep Recognition Network</b><br>
                                Delong Zhu, Zhe Min, Tong Zhou, <b>Tingguang Li</b>, Max Q.-H. Meng.<br> 
                                (IEEE Transactions on Instrumentation & Measurement 2020) 
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr></tbody></table>           
                        </td>
                    </tr> 
                    </tbody>

                    <tbody><tr style="border-width: 1px">
                        <td>
                            <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                            <p>
                                <b>A Searching Space Constrained Partial to Full Registration Approach with Applications in Airport Trolley Deployment Robot</b><br>
                                Jin Pan, Xiaochun Mai, Chaoqun Wang, Zhe Min, Jiankun Wang, Hu Cheng, <b>Tingguang Li</b>, Erli Lyu, Li Liu, Max Q.-H. Meng.<br> 
                                (IEEE Sensor Journal 2020) 
                            </p>
                            </td><td style="width: 10px;">
                            </td>
                            </tr></tbody></table>           
                        </td>
                    </tr> 
                    </tbody>

                    
                </table>

                <br><br><h1>Conference Papers</h1>
                    <table id="portfolio-projects">
                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/world_model.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>An Efficient Model-Based Approach on Learning Agile Motor Skills without Reinforcement</b><br>
                                    Haojie Shi*, <b>Tingguang Li*</b>, Qingxu Zhu, Jiapeng Sheng, Lei Han, Max Q.-H. Meng.<br> 
                                    (IEEE International Conference on Robotics and Automation, <b>ICRA 2024</b>)
                                    <br><a href="https://arxiv.org/abs/2403.01962">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=2R4RffrzS98">Video Demo (YouTube)</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

                        <table id="portfolio-projects">
                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/dynamic.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Learning Highly Dynamic Behaviors for Quadrupedal Robots</b><br>
                                    Chong Zhang, Jiapeng Sheng, <b>Tingguang Li</b>, He Zhang, Cheng Zhou, Qingxu, Zhu, Rui Zhao, Yizheng Zhang, Lei Han.<br> 
                                    (IEEE International Conference on Robotics and Automation, <b>ICRA 2024</b>) 
                                    <br><a href="https://arxiv.org/abs/2402.13473">Paper</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

                        <table id="portfolio-projects">
                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/climb_stairs.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Learning Terrain-Adaptive Locomotion with Agile Behaviors by Imitating Animals</b><br>
                                    <b>Tingguang Li</b>, Yizheng Zhang, Chong Zhang, Qingxu Zhu, Jiapeng Sheng, Cheng Zhou, Lei Han.<br> 
                                    (IEEE/RSJ International Conference on Intelligent Robots and Systems, <b>IROS 2023</b>) 
                                    <br><a href="https://arxiv.org/abs/2308.03273">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=jvFK-JiriTg">Video Demo (YouTube)</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

                         <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/HouseExpo.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>HouseExpo: A Large-scale 2D Indoor Layout Dataset for Learning-based Algorithms on Mobile Robots</b><br>
                                    <b>Tingguang Li</b>, Danny Ho, Chenming Li, Delong Zhu, Chaoqun Wang, Max Q.-H. Meng.<br> 
                                    (IEEE/RSJ International Conference on Intelligent Robots and Systems, <b>IROS 2020</b>) 
                                    <br><a href="https://sites.google.com/view/houseexpo/home">Project Page</a>&nbsp;·&nbsp;<a href="https://arxiv.org/abs/1903.09845">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=v7XPzj62OfE&feature=youtu.be">Video Demo (YouTube)</a>&nbsp;·&nbsp;<a href="https://github.com/TeaganLi/HouseExpo">Code (Github)</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 


 						<tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/PoseSpace.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Learning Hierarchical Control for Robust In-Hand Manipulation</b><br>
                                    <b>Tingguang Li</b>, Krishnan Srinivasan, Max Q.-H. Meng, Wenzhen Yuan, Jeannette Bohg. <br> ( IEEE International Conference on Robotics and Automation, <b>ICRA 2020</b>)
                                    <br><a href="https://sites.google.com/view/learninghierarchicalcontrol/home">Project Page</a>&nbsp;·&nbsp;<a href="https://arxiv.org/abs/1910.10985">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?time_continue=8&v=s8j2b79ByuQ">Video Demo (YouTube)</a>·&nbsp;<a href="https://github.com/TeaganLi/ICRA2020_manipulation">Code (Github)</a>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

						<tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/rubikcube.jpg"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Solving a Rubik's Cube with a Dexterous Hand</b><br>
                                    <b>Tingguang Li</b>, Weitao Xi, Meng Fang, Jia Xu, Max Q.-H. Meng. 
                                    <br>
                                    (IEEE International Conference on Robotics and Biomimetics, <b>ROBIO 2019</b>)
                                    <br><a href="https://sites.google.com/view/learning-solve-a-rubiks-cube/home">Project Page</a>&nbsp;·&nbsp;<a href="https://arxiv.org/abs/1907.11388">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=4st_54rqJB0&t=5s">Video Demo (YouTube)</a>&nbsp;·&nbsp;<a href="https://github.com/TeaganLi/RubikCube-InHandManipulation">Code (Github)</a>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

						<tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/ElevatorButton.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>A Novel OCR-RCNN for Elevator Button Recognition</b><br>
                                    Delong Zhu, <b>Tingguang Li</b>, Danny Ho, Tong Zhou, Max Q.-H. Meng. <br> (IEEE/RSJ International Conference on Intelligent Robots and Systems, <b>IROS 2018</b>)				<br><a href="https://ieeexplore.ieee.org/abstract/document/8594071">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=2iRoNHpzLl0">Video Demo (YouTube)</a>&nbsp;·&nbsp;<a href="https://github.com/zhudelong/elevator_button_recognition">Code (Github)</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

						

                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/combo.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Deep Reinforcement Learning Supervised Autonomous Exploration in Office Environment</b><br>
                                    Delong Zhu*, <b>Tingguang Li*</b>, Danny Ho*, Chaoqun Wang, Max Q.-H. Meng. <br> ( IEEE International Conference on Robotics and Automation, <b>ICRA 2018</b>)<br><a href="https://ieeexplore.ieee.org/abstract/document/8463213">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=zNc_mO1Is2Y&t=155s">Video Demo (YouTube)</a> *indicates equal contribution.
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr>   

                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/HRL.png"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Learning to Interrupt: A Hierarchical Deep Reinforcement Learning Framework for Efficient Exploration</b><br>
                                    <b>Tingguang Li</b>, Jin Pan, Delong Zhu, Max Q.-H. Meng.<br>
                                    (IEEE International Conference on Robotics and Biomimetics, <b>ROBIO 2018</b>)<br>
                                    <a href="https://ieeexplore.ieee.org/abstract/document/8665177">Paper</a>                           
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

       					<tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/thumbnail.jpg"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>A Hybrid 3DoF Pose Estimation Method Based on Camera and Lidar Data</b><br>
                                    <b>Tingguang Li</b>, Delong Zhu, Max Q.-H. Meng. <br> (IEEE International Conference on Robotics and Biomimetics, <b>ROBIO 2017, Best Conference Paper Award Finalist</b>)
                                    <br><a href="https://sites.google.com/view/hybrid-pose-estimation/home">Project Page</a>&nbsp;·&nbsp;<a href="https://ieeexplore.ieee.org/abstract/document/8324444">Paper</a>&nbsp;·&nbsp;
                                    <a href="https://www.youtube.com/watch?v=FqbH4VPqiyY&t=198s">Video Demo (YouTube)</a>&nbsp;·&nbsp;
                                    <a href="https://github.com/TeaganLi/A-Hybrid-3DoF-Pose-Estimation-Method-based-on-Camera-and-Lidar-Data">Code (Github)</a>
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr>       
                    </tbody>

			<tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/EMBC18.jpg"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>Bone Drilling Breakthrough Detection via Energy-based Signal</b><br>
                                    Danny Ho, <b>Tingguang Li</b>, Max Q.-H. Meng. <br> (International Conference of the IEEE Engineering in Medicine and Biology Society, <b>EMBC 2018</b>)<br><a href="./TingguangLi_files/embc2018.pdf">Paper</a>                           
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                        </tr> 

</table>

		    


                    <br><br><h1>Research Projects</h1>
                    <table id="portfolio-projects">

                        <tbody><tr style="border-width: 1px">
                            <td><img src="./TingguangLi_files/icra17RoboMaster.jpg"></td>
                            <td>
                                <table style="width: 100%;"><tbody><tr><td style="width: 100%; text-align: left;">
                                <p>
                                    <b>ICRA 2017 DJI RoboMaster Manipulation Challenge (5/93)</b><br>
                                    Developed a mobile manipulator which can autonomously detect, pick, transport, and stack the target cubes. In the final competition we successfully piled up 10 cubes and ranked 5th place.
                                    <br><a href="https://www.youtube.com/watch?v=49W8D0tlCAw">Video Demo (YouTube)</a> 
                                </p>
                                </td><td style="width: 10px;">
                                </td>
                                </tr></tbody></table>           
                            </td>
                       	</tr>   

                    </table>

                <br><br><h1>Talks</h1>
                <br><ul style="list-style-type:disc; margin-left: 20px">
                    <li>将门AI红人榜专访：<a href="https://mp.weixin.qq.com/s/xDg2EtJQyiIMVny5NGuD3w">“爱折腾”与“慢慢来”</a></li>
                    <li>将门Talk：<a href="https://mp.weixin.qq.com/s/zgtneimH0VbHKeNc8WjDTA">一种基于层次强化学习的机械手鲁棒操作</a></li>
                    <li>One paper <em>Solving a Rubik's Cube with a Dexterous Hand</em> is reported by <a href=https://techxplore.com/news/2019-08-rubik-cube-dexterous.html>TechXplore</a>.</li>
                </ul>


                <br><br><h1>Honors</h1>
                <br><ul style="list-style-type:disc; margin-left: 20px">
                    <li>Tencent Technology Breakthrough Award </a> (2020, 2022)</li>
                    <li>Outstanding Tutor Award of The Chinese University of Hong Kong</a> (2020)</li>
                    <li>Outstanding Students Award of The Chinese University of Hong Kong</a> (2018)</li>
                    <li>IROS 2018 travel award</a> (2018)</li>
                    <li>ICRA 2018 student travel award</a> (2018)</li>
                    <li>Best Conference Paper Award Finalist of <a href="http://2017.ieee-robio.org/awards/">ROBIO</a> (2017)</li>
                    <li>The 5th place winner among 93 teams at the worldwide <a href="http://www.icra2017.org/conference/robot-challenges">ICRA 2017 DJI RoboMaster Manipulation Challenge</a> (2017)</li>
                    <li>Hong Kong Ph.D. Fellowship (2016 - 2020)</li>
                    <li>Outstanding Undergraduate Award of Nanjing University (2016)</li>
                    <li>National Undergraduate Scholarship (2015)</li>                    
                </ul>
                
                <br><br><br><h1>Teaching</h1>
                <br><ul style="list-style-type:disc; margin-left: 20px">
                	<li>Teaching Assistant: CUHK ELEG4407, Intelligent Interactive Robot Practice, 2019</li>
                    <li>Teaching Assistant: CUHK ELEG 2201, Digital Circuits and Systems, 2018</li>
                    <li>Teaching Assistant: CUHK ELEG 2202, Fundamentals of Electric Circuits, 2018</li>
                    <li>Teaching Assistant: CUHK BMEG 4130, Biomedical Modeling, 2016</li>
                </ul>

                <br><br><br><br><br>
            </div>
		</div>	
	
</body></html>
